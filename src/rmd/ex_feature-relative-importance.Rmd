---
title: Variable importance in ANNs
output: html_document
---

```{r setup, include=FALSE}
source(file.path(PROJHOME, "src/r/setup.R"))
setup()
```

Examples are from the following:

> Ibrahim, OM. 2013. [A comparison of methods  for  assessing the  relative  importance of input variables in  artificial neural networks](http://www.palisade.com/downloads/pdf/academic/DTSpaper110915.pdf). Journal of Applied Sciences Research, 9(11): 5692-5700.

Table 4: Final connection weights
```{r network-weights}
w <- NULL
w$ih <-
  data.frame(x1 = c(-0.8604, 1.0651, -0.1392, 1.0666),
             x2 = c(-6.6167, 1.5142, -5.3051, 1.3358),
             x3 = c(-1.7338, 1.4107, -1.4207, 0.7845)) %>%
  t() %>%
  `colnames<-`(c("h1", "h2", "h3","h4"))

w$ho <-
  data.frame(gy = c(-5.2075, 2.0158, -4.4617, 1.5858)) %>%
  t() %>%
  `colnames<-`(c("h1", "h2", "h3","h4"))

w
```

<br>
<br>

### Connection weights algorithm (CW)

$RI_x = \sum_{y=1}^m{w_{xy}w_{yz}}$

----

Here we show explicitly the calculations for input `x1`

```{r echo=TRUE}
# We first calculate the connection weight product of eg. input x1
# w_x1,h1 * w_h1,gy
# w_x1,h2 * w_h2,gy
# w_x1,h3 * w_h3,gy
# w_x1,h4 * w_h4,gy
(cw_prd_x1 <- w$ih[1,] * w$ho)

# & then take the sum
sum(cw_prd_x1)
```

----

We do the calculation for all inputs and get the following connection weight product sums (`cw_prd_sums`):

```{r}
(cw_prd_sums <-
  sapply(1:3, function(i){sum(w$ih[i,] * w$ho)}) %>%
  setNames(., rownames(w$ih))
 )
```

To calculate relative importance, we normalize and sort to find the most important feature

```{r echo=TRUE}
(cw_prd_sums / sum(cw_prd_sums)*100) %>%
  sort(decreasing = TRUE)
```

<br>
<br>

### The first proposed algorithm (MCW) - Modified Connection Weights

$RI_x = \frac{|\sum_{y=1}^m{w_{xy}w_{yz} \times r_{ij.k}}|}{\sum_{x=1}^n{\sum_{y=1}^m{w_{xy}w_{yz}}}}$

*A correction term (partial correlation) is multiplied by this sum and the absolute value is taken, this is called the corrected sum. Then to calculate the relative importance of each input, the corrected sum of each input is divided by the total corrected sum.*

<br>

#### Partial correlation

$r_{ij.k}=\frac{r_{ij} - r_{ki} \times r_{kj}}{\sqrt{(1-r_{ki}^2) \times (1-r_{kj}^2)}}$

*[Partial correlation](https://en.wikipedia.org/wiki/Partial_correlation) measures the degree of association between two random variables, with the effect of a set of controlling random variables removed.* -- wikipedia

----

The following is from [SlideShare: What is a partial correlation?](https://www.slideshare.net/plummer48/what-is-a-partial-correlation) (Ken Plummer, 2014)

```{r}
(dat <-
  data.frame(
    individual = letters[1:8],
    height = c(73, 70, 69, 68, 70, 68, 67, 62),
    weight = c(240, 210, 180, 160, 150, 140, 135, 120),
    gender = c(rep(1, 4), rep(2, 4)),
    stringsAsFactors = FALSE
    )
 )
```

The correlation between height and weight is 0.825

```{r echo=TRUE}
cor(dat$height, dat$weight, method="pearson") #default is pearson
```

However, when controlling for gender (male=1; female=2) the correlation between height and weight is 0.770

```{r echo=TRUE, results='hold'}
(r_ij <- cor(dat$height, dat$weight))
(r_ki <- cor(dat$gender, dat$height))
(r_kj <- cor(dat$gender, dat$weight))

numer <- r_ij-(r_ki*r_kj)
denom <- sqrt((1-r_ki^2) * (1-r_kj^2))

sprintf("Partial correlation: %0.3f", numer/denom)
```


> A partial correlation makes it possible to eliminate the effect of a third variable

----
